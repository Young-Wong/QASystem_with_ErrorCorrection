{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    import json\n",
    "    import jsonpath\n",
    "    \n",
    "    with open('train-v2.0.json', 'r', encoding='utf-8') as f:\n",
    "        dic = json.load(f)\n",
    "    \n",
    "    qlist = jsonpath.jsonpath(dic,'$..question') \n",
    "    alist = jsonpath.jsonpath(dic,'$..text') \n",
    "    f.close()\n",
    "    assert len(qlist) == len(alist) \n",
    "    \n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53169\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "qlist, alist = read_corpus()\n",
    "\n",
    "qwords = []\n",
    "for sentence in qlist:\n",
    "    qwords.append(word_tokenize(sentence))\n",
    "\n",
    "mydist = {}\n",
    "for sentence in qwords:\n",
    "    for word in sentence:\n",
    "        if word not in mydist:\n",
    "            mydist[word] =1\n",
    "        else:\n",
    "            mydist[word] +=1\n",
    "\n",
    "word_total = len(mydist)\n",
    "print (word_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "f_list = []\n",
    "english_punctuations = ['(', ')', '[', ']', '&', '!!', '*', '@', '#', '$', '%', '?']\n",
    "\n",
    "def sentencesList_preprocess(sentencesList):\n",
    "\n",
    "    for sentence in sentencesList:\n",
    "        # filter stop words\n",
    "        filtered = [word for word in sentence if word not in stop_words]\n",
    "        \n",
    "        # to lower case\n",
    "        lower_case = [word.lower() for word in filtered]\n",
    "        \n",
    "        # lemmatization\n",
    "        lemma = [wnl.lemmatize(token) for token in lower_case]\n",
    "        \n",
    "        # remove unused symbols\n",
    "        sym = [word for word in lemma if word not in english_punctuations]\n",
    "        \n",
    "        f_list.append(sym)\n",
    "    \n",
    "    return f_list\n",
    "\n",
    "qlist = sentencesList_preprocess(qwords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_preprocess(sentence):\n",
    "    # filter stop words\n",
    "    filtered = [word for word in sentence if word not in stop_words]\n",
    "    # to lower case\n",
    "    lower_case = [word.lower() for word in filtered]\n",
    "    # lemmatization\n",
    "    lemma = [wnl.lemmatize(token) for token in lower_case]\n",
    "    # remove unused symbols\n",
    "    token = [word for word in lemma if word not in english_punctuations]\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()   # define a tf-idf vectorizer\n",
    "\n",
    "qlistnew = [' '.join(sentence) for sentence in qlist]\n",
    "X_tfidf = vectorizer.fit_transform(qlistnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Glove\n",
    "def loadGlove(path):\n",
    "    vocab = {}\n",
    "    embedding = []\n",
    "    vocab[\"UNK\"] = 0\n",
    "    embedding.append([0]*200)\n",
    "    file = open(path, 'r', encoding='utf8')\n",
    "    i = 1\n",
    "    for line in file:\n",
    "        row = line.strip().split()\n",
    "        vocab[row[0]] = i\n",
    "        embedding.append(row[1:])\n",
    "        i += 1\n",
    "    print(\"Finish load Glove\")\n",
    "    file.close()\n",
    "    return vocab, embedding\n",
    "\n",
    "path = '../glove.6b/glove.6b.200d.txt'\n",
    "voc, emb = loadGlove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vec(embedding, sentence):\n",
    "    vec = np.zeros((200,), dtype=np.float64)\n",
    "    for word in sentence:\n",
    "        if word in voc:\n",
    "            idx = voc[word]\n",
    "            vec += embedding[idx].astype('float64')\n",
    "        else:\n",
    "            vec += embedding[0].astype('float64')\n",
    "    vec = vec/len(sentence)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_vec(embedding, sentences):\n",
    "    vec = np.zeros((len(sentences), 200))\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence = sentence.strip().split(' ')\n",
    "        vec[i] = sentence_to_vec(embedding, sentence)\n",
    "                \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emc = np.asarray(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_w2v = sentences_to_vec(emc, qlistnew) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# return the top k numbers from the list using prority queue\n",
    "def get_top_numbers(tlist, k):\n",
    "    max_heap = []\n",
    "    l = len(tlist)\n",
    "    if l<=0 or k<=0 or k>l: return None\n",
    "    \n",
    "    for i in tlist:\n",
    "        if k > len(max_heap): heapq.heappush(max_heap, i)\n",
    "        else: heapq.heappushpop(max_heap, i)\n",
    "    \n",
    "    return max_heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_results_tfidf_noindex(query):\n",
    "\n",
    "    query = word_tokenize(query)\n",
    "    query= sentence_preprocess(query)\n",
    "    query = ' '.join(query)\n",
    "    \n",
    "    q_tfidf = vectorizer.transform([query])\n",
    "    res = list(cosine_similarity(q_tfidf, X_tfidf)[0])\n",
    "    result = sorted(get_top_numbers(res, 5), reverse = True)\n",
    "    \n",
    "    top_idxs = []  \n",
    "    dict_visited = {}\n",
    "\n",
    "    for r in result:\n",
    "        for i, n in enumerate(res):\n",
    "            if n == r and i not in dict_visited: \n",
    "                top_idxs.append(i)\n",
    "                dict_visited[i] = True\n",
    "\n",
    "    ans = [alist[i] for i in top_idxs]\n",
    "    \n",
    "    return ans    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (get_top_results_tfidf_noindex(\"when did Beyonce start becoming popular\"))\n",
    "print (get_top_results_tfidf_noindex(\"what languge does the word of 'symbiosis' come from\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an inverted index\n",
    "inverted_idx = {}  \n",
    "for index, sentence in enumerate(qlist):\n",
    "    for word in sentence:\n",
    "        if word not in inverted_idx: inverted_idx[word] = [index]\n",
    "        else: inverted_idx[word].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_words(file):\n",
    "    related_words = {}\n",
    "    for line in open(file, mode='r', encoding='utf-8'):\n",
    "        item = line.split(\",\")\n",
    "        word, s_list = item[0], [value for value in item[1].strip().split()]\n",
    "        related_words[word] = s_list\n",
    "    return related_words\n",
    "\n",
    "related_words = get_related_words('related_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inverted_index_sentence(query):\n",
    "    query = word_tokenize(query)\n",
    "    query= sentence_preprocess(query)\n",
    "    \n",
    "    r_list = []\n",
    "    for q in query:\n",
    "        if q in related_words: \n",
    "            for word in related_words[q]:\n",
    "                r_list.append(word)\n",
    "    \n",
    "    total_list = query\n",
    "    for word in r_list:\n",
    "        total_list.append(word)\n",
    "    \n",
    "    idx_list = []    \n",
    "    for word in total_list:\n",
    "        if word in inverted_idx:\n",
    "            indx = inverted_idx[word]\n",
    "            idx_list.extend(indx)\n",
    "    return query, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_index(result):\n",
    "    top_idxs = []\n",
    "    dict_visited = {}\n",
    "    top_result = sorted(get_top_numbers(result, 5), reverse = True)\n",
    "    \n",
    "    for r in top_result:\n",
    "        for i, n in enumerate(result):\n",
    "            if n == r and i not in dict_visited: \n",
    "                top_idxs.append(i)\n",
    "                dict_visited[i] = True\n",
    "    return top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_results_tfidf(query):\n",
    "\n",
    "    query, sentence_idxs = get_inverted_index_sentence(query)\n",
    "    query = ' '.join(query)\n",
    "    q_tfidf = vectorizer.transform([query])\n",
    "    \n",
    "    \n",
    "    X_tfidf_idx = []\n",
    "    for indx in sentence_idxs:\n",
    "        X_tfidf_idx.append(X_tfidf[indx].toarray()[0])\n",
    "    \n",
    "    res = list(cosine_similarity(q_tfidf, X_tfidf_idx)[0])\n",
    "    \n",
    "    top_idxs = []   \n",
    "    top_idxs = get_top_index(res)\n",
    "    \n",
    "    ans = [alist[i] for i in top_idxs]\n",
    "    \n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_query1 = \"when did Beyonce start becoming popular\"\n",
    "test_query2 = \"what languge does the word of symbiosis come from\"\n",
    "\n",
    "print (get_top_results_tfidf(test_query1))\n",
    "print (get_top_results_tfidf(test_query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_results_w2v(query):\n",
    "    \"\"\"\n",
    "    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "    1. 利用倒排表来筛选 candidate （需要使用related_words). \n",
    "    2. 对于候选文档，计算跟输入问题之间的相似度\n",
    "    3. 找出相似度最高的top5问题的答案\n",
    "    \"\"\"\n",
    "        \n",
    "    query, sentence_idxs = get_inverted_index_sentence(query)\n",
    "    query = ' '.join(query)\n",
    "    q_w2v = sentence_to_vec(emc,query)\n",
    "    \n",
    "    \n",
    "    q_w2v_idx = []\n",
    "    for indx in sentence_idxs:\n",
    "        q_w2v_idx.append(X_w2v[indx])\n",
    "    \n",
    "    res = list(cosine_similarity(np.array([q_w2v]), np.array([q_w2v_idx])[0])[0])\n",
    "    \n",
    "    top_idxs = []   \n",
    "    top_idxs = get_top_index(res)\n",
    "    \n",
    "    ans = [alist[i] for i in top_idxs]\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_query1 = \"when did Beyonce start becoming popular\"\n",
    "test_query2 = \"what languge does the word of symbiosis come from\"\n",
    "\n",
    "print (get_top_results_tfidf(test_query1))\n",
    "print (get_top_results_w2v(test_query1))\n",
    "# print (get_top_results_bert(test_query1))\n",
    "\n",
    "print (get_top_results_tfidf(test_query2))\n",
    "print (get_top_results_w2v(test_query2))\n",
    "# print (get_top_results_bert(test_query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import codecs\n",
    "# 读取语料库的数据\n",
    "categories = reuters.categories()\n",
    "corpus = reuters.sents(categories=categories)\n",
    "\n",
    "word2index = {}\n",
    "index2word = {}\n",
    "corpus2 = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    corpus2.append(['<s> '] + sentence + [' </s>'])\n",
    "\n",
    "for sentence in corpus2:\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word in word2index: continue\n",
    "        index2word[len(word2index)] = word\n",
    "        word2index[word] = len(word2index)\n",
    "\n",
    "word_count = len(word2index)\n",
    "uni_count = np.zeros(word_count)\n",
    "bi_count = np.zeros((word_count, word_count))\n",
    "\n",
    "for sentence in corpus2:\n",
    "    for i, word in enumerate(sentence):\n",
    "        word = word.lower()\n",
    "        uni_count[word2index[word]] += 1\n",
    "        if i <len(sentence) -1:\n",
    "            pre = word2index[word]\n",
    "            curr = word2index[sentence[i+1].lower()]\n",
    "            bi_count[pre, curr] +=1\n",
    "\n",
    "bigram = np.zeros((word_count, word_count))\n",
    "\n",
    "for i in range(word_count):\n",
    "    for j in range(word_count):\n",
    "        if bi_count[i,j]==0:\n",
    "            bigram[i,j] = 1.0 / (uni_count[i] + word_count)\n",
    "        else:\n",
    "            bigram[i,j] = (1.0 + bi_count[i,j]) / (word_count + uni_count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkCount(pre,word):\n",
    "    if pre.lower() in word2index and word.lower() in word2index:\n",
    "        return bigram[word2index[pre.lower()],word2index[word.lower()]]\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the channel probability  \n",
    "channel = {}\n",
    "\n",
    "spell_error_dict = {}\n",
    "for line in open('spell-errors.txt'):\n",
    "    word = line.split(\":\")\n",
    "    c_word = word[0] # correct word is the key\n",
    "    spell_error_dict[c_word] = [e_word.strip( )for e_word in word[1].strip().split(\",\")]\n",
    "\n",
    "# TODO\n",
    "for c_word in spell_error_dict:\n",
    "    if c_word not in channel:\n",
    "        channel[c_word] = {}\n",
    "    for e_word in spell_error_dict[c_word]:\n",
    "        channel[c_word][e_word] = 1/len(spell_error_dict[c_word])\n",
    "    \n",
    "# print(channel)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(w for w in words if w in word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    #删除\n",
    "    s1 = [word[0:i] + word[i+1:] for i in range(n)]\n",
    "    #调换相连的两个字母\n",
    "    s2 = [word[0:i] + word[i+1] + word[i] + word[i+2:] for i in range(n-1)]\n",
    "    #replace\n",
    "    s3 = [word[0:i] + c + word[i + 1:] for i in range(n) for c in alphabet]\n",
    "    #插入\n",
    "    s4 = [word[0:i] + c + word[i:] for i in range(n + 1) for c in alphabet]\n",
    "    edit1_words = set(s1 + s2 + s3 + s4)\n",
    "\n",
    "    if word in edit1_words:\n",
    "        edit1_words.remove(word)\n",
    "\n",
    "    edit1_words = known(edit1_words)\n",
    "    return edit1_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits2(word, edit1_words):\n",
    "    edit2_words = set(e2 for e1 in edit1_words for e2 in edits1(e1))\n",
    "    \n",
    "    if word in edit2_words:\n",
    "        edit2_words.remove(word)\n",
    "    \n",
    "    edit2_words = known(edit2_words)\n",
    "    return edit2_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates(word):\n",
    "    # generate the words that have edit distance of 1 or 2 \n",
    "    \n",
    "    word_edit1 = edits1(word)\n",
    "    word_edit2 = edits2(word, word_edit1)\n",
    "    \n",
    "    words = word_edit1 | word_edit2\n",
    "    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from queue import PriorityQueue\n",
    "def spell_corrector(line):\n",
    "    # 1. 首先做分词，然后把``line``表示成``tokens``\n",
    "    # 2. 循环每一token, 然后判断是否存在词库里。如果不存在就意味着是拼写错误的，需要修正。 \n",
    "    #    修正的过程就使用上述提到的``noisy channel model``, 然后从而找出最好的修正之后的结果。  \n",
    "    \n",
    "    corrected_words = []\n",
    "    tokens = []\n",
    "    tokens = ['<s>']+word_tokenize(line)+['</s>']\n",
    "    for i, token in enumerate(tokens):\n",
    "        if i == len(tokens)-1: break\n",
    "        if token.lower() not in word2index:\n",
    "            pre, nxt = tokens[i-1].lower(), tokens[i+1].lower()\n",
    "            token = word_corrector(token, pre, nxt)\n",
    "            corrected_words.append(token)\n",
    "        else: corrected_words.append(token)\n",
    "    newline = ' '.join(corrected_words)\n",
    "    \n",
    "    return newline   # 修正之后的结果，假如用户输入没有问题，那这时候``newline = line``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_corrector(word, pre_word, next_word):\n",
    "    candidates = generate_candidates(word)\n",
    "    correctors = PriorityQueue()\n",
    "    \n",
    "    if len(candidates) == 0: return word\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if candidate in channel and word in channel[candidate] and candidate in word2index:\n",
    "            bi_pre = checkCount(pre_word, candidate)\n",
    "            bi_nxt = checkCount(candidate, next_word)\n",
    "            p = np.log(channel[candidate][word] + 0.001) + bi_pre + bi_nxt\n",
    "            correctors.put((-1*p, candidate))\n",
    "    \n",
    "    if correctors.empty(): return word\n",
    "    \n",
    "    return correctors.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_query1 = \"What counted for more of the poplation change\"  # 拼写错误的\n",
    "test_query2 = \"What counted for more of the population chenge\"  # 拼写错误的\n",
    "\n",
    "test_query1 = spell_corrector(test_query1)\n",
    "test_query2 = spell_corrector(test_query2)\n",
    "\n",
    "print(test_query1)\n",
    "print(test_query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_query1 = \"\"  # 拼写错误的\n",
    "test_query2 = \"\"  # 拼写错误的\n",
    "\n",
    "test_query1 = spell_corector(test_query1)\n",
    "test_query2 = spell_corector(test_query2)\n",
    "\n",
    "print (get_top_results_tfidf(test_query1))\n",
    "print (get_top_results_w2v(test_query1))\n",
    "print (get_top_results_bert(test_query1))\n",
    "\n",
    "print (get_top_results_tfidf(test_query2))\n",
    "print (get_top_results_w2v(test_query2))\n",
    "print (get_top_results_bert(test_query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
